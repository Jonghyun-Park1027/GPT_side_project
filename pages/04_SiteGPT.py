import os
import pathlib
from datetime import datetime

import streamlit as st
from langchain.chat_models import ChatOpenAI
from langchain.chains import RetrievalQA
from langchain.document_loaders.sitemap import SitemapLoader
from langchain.document_loaders import WebBaseLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from langchain.vectorstores import Chroma


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Sidebar â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.sidebar.header("ğŸ¤– Cloudflare SiteGPT")
api_key = st.sidebar.text_input("ğŸ”‘ OpenAI API Key", type="password")
st.sidebar.markdown(
    "[ğŸ“‚ GitHub Repository](https://github.com/your-github-id/cloudflare-sitegpt)"
)

if not api_key:
    st.info("ë¨¼ì € OpenAI API Keyë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    st.stop()

os.environ["OPENAI_API_KEY"] = api_key

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Settings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
SITEMAP_INDEX = "https://developers.cloudflare.com/sitemap-0.xml"
CF_PRODUCTS = {
    "AI Gateway": "ai-gateway",
    "Vectorize": "vectorize",
    "Workers AI": "workers-ai",
}
NOMAD_URL = "https://nomadcoders.co/c/gpt-challenge/lobby"

CACHE_DIR = pathlib.Path(".cache")
CACHE_DIR.mkdir(exist_ok=True)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Loader Functions â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€


def load_cf_docs(selected_slugs: list[str]):
    """Load Cloudflare docs from master sitemap filtered by product slugs"""
    patterns = [rf".*{slug}/.*" for slug in selected_slugs]
    loader = SitemapLoader(web_path=SITEMAP_INDEX, filter_urls=patterns)
    docs = loader.load()
    for d in docs:
        # Extract product name from URL path
        for name, slug in CF_PRODUCTS.items():
            if f"/{slug}/" in d.metadata.get("source", ""):
                d.metadata["product"] = name
                break
    return docs


def load_nomad_doc():
    loader = WebBaseLoader(NOMAD_URL)
    docs = loader.load()
    for d in docs:
        d.metadata["product"] = "Nomad GPT Challenge"
    return docs


def build_or_load_chroma(selected_products: list[str]):
    key = "_".join(sorted(selected_products)).replace(" ", "_").lower()
    store_path = CACHE_DIR / f"{key}_chroma"

    # Chroma uses a directory for persistence
    if store_path.exists() and any(store_path.iterdir()):
        return Chroma(
            persist_directory=str(store_path), embedding_function=OpenAIEmbeddings()
        )

    # --- Load docs ---
    cf_slugs = [CF_PRODUCTS[p] for p in selected_products if p in CF_PRODUCTS]
    docs = load_cf_docs(cf_slugs)
    if "Nomad GPT Challenge" in selected_products:
        docs += load_nomad_doc()

    # --- Split & embed ---
    splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=150)
    chunks = splitter.split_documents(docs)
    vectordb = Chroma.from_documents(
        chunks, OpenAIEmbeddings(), persist_directory=str(store_path)
    )
    vectordb.persist()
    return vectordb


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ UI Inputs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
available_products = list(CF_PRODUCTS.keys()) + ["Nomad GPT Challenge"]
selected_products = st.sidebar.multiselect(
    "ëŒ€ìƒ ë¬¸ì„œ", available_products, default=available_products
)


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Retriever â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_resource(show_spinner=True, ttl=24 * 3600)
def get_retriever(products):
    vectordb = build_or_load_chroma(products)
    return vectordb.as_retriever(search_type="similarity", search_kwargs={"k": 4})


retriever = get_retriever(selected_products)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ QA Chain â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
llm = ChatOpenAI(temperature=0)
qa_chain = RetrievalQA.from_chain_type(
    llm=llm, retriever=retriever, return_source_documents=True
)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main Area â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("Cloudflare Docs GPT ğŸ¤–")
query = st.text_input("Cloudflare ê³µì‹ ë¬¸ì„œì— ëŒ€í•´ ì§ˆë¬¸í•˜ì„¸ìš”:")

if query:
    with st.spinner("ë‹µë³€ ìƒì„± ì¤‘..."):
        result = qa_chain(query)
        st.subheader("ğŸ“˜ ë‹µë³€")
        st.write(result["result"])

        with st.expander("ğŸ” ì°¸ê³  ë¬¸ì„œ"):
            for doc in result["source_documents"]:
                url = doc.metadata.get("source", "")
                title = doc.metadata.get("title", url)
                st.markdown(f"- [{title}]({url})")

st.markdown("---")
st.markdown("ì˜ˆì‹œ ì§ˆë¬¸:")
st.markdown("- llama-2-7b-chat-fp16 ëª¨ë¸ì˜ 1M ì…ë ¥ í† í°ë‹¹ ê°€ê²©ì€ ì–¼ë§ˆì¸ê°€ìš”?")
st.markdown("- Cloudflareì˜ AI ê²Œì´íŠ¸ì›¨ì´ë¡œ ë¬´ì—‡ì„ í•  ìˆ˜ ìˆë‚˜ìš”?")
st.markdown("- ë²¡í„°ë¼ì´ì¦ˆì—ì„œ ë‹¨ì¼ ê³„ì •ì€ ëª‡ ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë‚˜ìš”?")

st.caption(f"Index last updated: {datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S UTC')}")
